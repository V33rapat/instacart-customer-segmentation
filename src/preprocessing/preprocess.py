"""
Simple preprocessing script - ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Missing Values, Outliers, Scaling
"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
import pickle
from pathlib import Path
import warnings

warnings.filterwarnings('ignore')

def preprocess_merged_data(input_path='data/merged_data.csv', 
                          output_path='data/preprocessed/processed_data.csv',
                          scaler_path='data/preprocessed/scaler.pkl',
                          sample_size=None):
    """
    ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏ï‡πá‡∏°‡∏£‡∏π‡∏õ:
    1. ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
    2. ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Missing Values
    3. ‡∏ï‡∏£‡∏ß‡∏à‡∏´‡∏≤‡πÅ‡∏•‡∏∞‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Outliers
    4. Standard Scaling
    5. ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
    """
    
    print("="*70)
    print("üìä DATA PREPROCESSING")
    print("="*70)
    
    # 1. ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
    print("\n[1/5] ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•...")
    if sample_size:
        df = pd.read_csv(input_path, nrows=sample_size)
        print(f"‚úÖ ‡πÇ‡∏´‡∏•‡∏î {sample_size} rows: {df.shape}")
    else:
        df = pd.read_csv(input_path)
        print(f"‚úÖ ‡πÇ‡∏´‡∏•‡∏î‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {df.shape}")
    
    original_shape = df.shape
    
    # 2. ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Missing Values
    print("\n[2/5] ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Missing Values...")
    missing_before = df.isnull().sum()
    missing_cols = missing_before[missing_before > 0]
    
    if len(missing_cols) > 0:
        print(f"   ‡∏û‡∏ö missing ‡πÉ‡∏ô {len(missing_cols)} ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå:")
        for col, count in missing_cols.items():
            pct = count / len(df) * 100
            print(f"   - {col}: {count} ({pct:.2f}%)")
        
        # ‡πÄ‡∏ï‡∏¥‡∏° missing ‡∏î‡πâ‡∏ß‡∏¢ mean
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        imputer = SimpleImputer(strategy='mean')
        df[numeric_cols] = imputer.fit_transform(df[numeric_cols])
        print(f"   ‚úÖ ‡πÄ‡∏ï‡∏¥‡∏°‡∏î‡πâ‡∏ß‡∏¢ mean strategy")
    else:
        print(f"   ‚úÖ ‡πÑ‡∏°‡πà‡∏°‡∏µ missing values")
    
    # 3. ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Outliers (IQR method)
    print("\n[3/5] ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Outliers (IQR method)...")
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    outlier_count = 0
    
    for col in numeric_cols:
        Q1 = df[col].quantile(0.25)
        Q3 = df[col].quantile(0.75)
        IQR = Q3 - Q1
        
        if IQR > 0:
            lower = Q1 - 1.5 * IQR
            upper = Q3 + 1.5 * IQR
            
            outliers = ((df[col] < lower) | (df[col] > upper)).sum()
            if outliers > 0:
                outlier_count += outliers
                df[col] = df[col].clip(lower=lower, upper=upper)
    
    print(f"   ‚úÖ ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ {outlier_count} outliers (clipped)")
    
    # 4. Standard Scaling (‡∏¢‡∏Å‡πÄ‡∏ß‡πâ‡∏ô ID columns)
    print("\n[4/5] ‡∏ó‡∏≥ Standard Scaling...")
    # ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå ID ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á scale
    id_cols = ['user_id', 'order_id', 'product_id']
    cols_to_scale = [col for col in numeric_cols if col not in id_cols]
    
    if len(cols_to_scale) > 0:
        scaler = StandardScaler()
        df[cols_to_scale] = scaler.fit_transform(df[cols_to_scale])
        print(f"   ‚úÖ Scaled {len(cols_to_scale)} feature columns")
        print(f"   ‚è≠Ô∏è  ‡∏ó‡∏≥‡πÄ‡∏ß‡πâ‡∏ô ID columns: {', '.join([c for c in id_cols if c in df.columns])}")
    else:
        scaler = StandardScaler()
        print(f"   ‚è≠Ô∏è  ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á scale")
    
    # 5. ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
    print("\n[5/5] ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå...")
    
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á output directory
    output_dir = Path(output_path).parent
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å processed data
    df.to_csv(output_path, index=False)
    print(f"   ‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å: {output_path}")
    print(f"      Shape: {df.shape}")
    
    # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å scaler
    with open(scaler_path, 'wb') as f:
        pickle.dump(scaler, f)
    print(f"   ‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å scaler: {scaler_path}")
    
    # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô
    report_path = output_dir / 'preprocessing_report.txt'
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write("DATA PREPROCESSING REPORT\n")
        f.write("="*70 + "\n\n")
        f.write(f"Original shape: {original_shape}\n")
        f.write(f"Final shape: {df.shape}\n\n")
        f.write(f"Preprocessing steps:\n")
        f.write(f"1. Missing Values: {len(missing_cols)} columns handled\n")
        f.write(f"2. Outliers: {outlier_count} values clipped (IQR method)\n")
        f.write(f"3. Scaling: StandardScaler applied to feature columns (excluded: user_id, order_id, product_id)\\n\\n")
        f.write(f"Output files:\n")
        f.write(f"- {output_path}\n")
        f.write(f"- {scaler_path}\n")
    
    print(f"   ‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô: {report_path}")
    
    print("\n" + "="*70)
    print("‚úÖ PREPROCESSING COMPLETE!")
    print("="*70)
    print(f"\nüìä ‡∏™‡∏£‡∏∏‡∏õ:")
    print(f"   Original: {original_shape}")
    print(f"   Processed: {df.shape}")
    print(f"   Missing values handled: {len(missing_cols)}")
    print(f"   Outliers clipped: {outlier_count}")
    print(f"\nüíæ Output files:")
    print(f"   - {output_path}")
    print(f"   - {scaler_path}")
    print(f"   - {report_path}\n")
    
    return df, scaler


if __name__ == '__main__':
    # ‡∏£‡∏±‡∏ô preprocessing
    df_processed, scaler = preprocess_merged_data(
        input_path='data/merged_data.csv',
        output_path='data/preprocessed/processed_data.csv',
        scaler_path='data/preprocessed/scaler.pkl',
        sample_size=None  # ‡πÉ‡∏ä‡πâ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î (‡∏´‡∏£‡∏∑‡∏≠‡πÉ‡∏™‡πà‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö sample)
    )
    
    print("üìå ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (5 rows ‡πÅ‡∏£‡∏Å):")
    print(df_processed.head())
    print(f"\nüìå Data types:")
    print(df_processed.dtypes)
